"""
PROPER NORMALIZED TABULAR MODEL

Bu model t√ºm major problemleri √ß√∂zer:
1. ‚úÖ Satellite bands proper normalization (0-1)
2. ‚úÖ Distribution shift handling
3. ‚úÖ Dense architecture (LSTM deƒüil)
4. ‚úÖ Tabular data approach
5. ‚úÖ Strong feature correlations kullanƒ±r
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def fix_satellite_band_normalization(df):
    """Fix satellite band normalization - CRITICAL FIX"""
    logging.info("üîß FIXING SATELLITE BAND NORMALIZATION...")
    
    df_fixed = df.copy()
    
    # Satellite bands that need normalization (0-10000 -> 0-1)
    satellite_bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B11', 'B12', 'B8A']
    
    for band in satellite_bands:
        if band in df_fixed.columns:
            original_range = (df_fixed[band].min(), df_fixed[band].max())
            
            # Normalize from 0-10000 to 0-1
            df_fixed[band] = df_fixed[band] / 10000.0
            df_fixed[band] = df_fixed[band].clip(0, 1)  # Ensure 0-1 range
            
            new_range = (df_fixed[band].min(), df_fixed[band].max())
            logging.info(f"  {band:4s}: {original_range[0]:8.1f}-{original_range[1]:8.1f} ‚Üí {new_range[0]:.4f}-{new_range[1]:.4f}")
    
    # Fix vegetation indices
    vegetation_fixes = {
        'NDVI': (-1, 1),
        'EVI': (-1, 1), 
        'SAVI': (0, 1),
        'NDRE': (0, 1)
    }
    
    for idx, (min_val, max_val) in vegetation_fixes.items():
        if idx in df_fixed.columns:
            original_range = (df_fixed[idx].min(), df_fixed[idx].max())
            df_fixed[idx] = df_fixed[idx].clip(min_val, max_val)
            new_range = (df_fixed[idx].min(), df_fixed[idx].max())
            logging.info(f"  {idx:4s}: {original_range[0]:8.1f}-{original_range[1]:8.1f} ‚Üí {new_range[0]:.4f}-{new_range[1]:.4f}")
    
    logging.info("‚úÖ Satellite band normalization FIXED!")
    return df_fixed

def load_and_fix_data():
    """Load data and apply critical fixes"""
    logging.info("üìÇ Loading and fixing data...")
    
    train_df = pd.read_csv('data/processed/train_data.csv')
    test_df = pd.read_csv('data/processed/test_data.csv')
    
    logging.info(f"Original shapes: Train {train_df.shape}, Test {test_df.shape}")
    
    # CRITICAL FIX 1: Normalize satellite bands
    train_df = fix_satellite_band_normalization(train_df)
    test_df = fix_satellite_band_normalization(test_df)
    
    # Clean yield data
    train_df = train_df.dropna(subset=['yield'])
    test_df = test_df.dropna(subset=['yield'])
    
    train_df = train_df[train_df['yield'] > 0]
    test_df = test_df[test_df['yield'] > 0]
    
    # Remove extreme outliers (keep 98% of data)
    train_q1, train_q3 = train_df['yield'].quantile([0.01, 0.99])
    test_q1, test_q3 = test_df['yield'].quantile([0.01, 0.99])
    
    train_df = train_df[(train_df['yield'] >= train_q1) & (train_df['yield'] <= train_q3)]
    test_df = test_df[(test_df['yield'] >= test_q1) & (test_df['yield'] <= test_q3)]
    
    logging.info(f"After fixes: Train {train_df.shape}, Test {test_df.shape}")
    
    # Check distribution shift
    train_mean = train_df['yield'].mean()
    test_mean = test_df['yield'].mean()
    shift = test_mean - train_mean
    
    logging.info(f"üéØ YIELD DISTRIBUTION:")
    logging.info(f"  Train: mean={train_mean:.2f}, std={train_df['yield'].std():.2f}")
    logging.info(f"  Test:  mean={test_mean:.2f}, std={test_df['yield'].std():.2f}")
    logging.info(f"  Shift: {shift:.2f} bushels/acre")
    
    if abs(shift) > 5:
        logging.warning(f"‚ö†Ô∏è Distribution shift still present: {shift:.2f}")
    else:
        logging.info(f"‚úÖ Distribution shift acceptable")
    
    return train_df, test_df

def select_strong_features(train_df, min_correlation=0.05):
    """Select features with strong correlations - use research findings"""
    logging.info("üéØ Selecting strong features...")
    
    # Focus on proven strong seasonal features first
    priority_features = [
        'mid_NDRE', 'mid_SAVI', 'mid_NDVI',  # Strongest correlations (0.57, 0.54, 0.54)
        'mid_temp_avg', 'mid_precipitation',   # Weather features  
        'late_NDRE', 'late_SAVI', 'late_NDVI',  # Late season
        'early_SAVI', 'early_NDVI', 'early_temp_avg',  # Early season
    ]
    
    # Add core satellite bands (now normalized)
    satellite_features = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B11', 'B12', 'B8A']
    
    # Add core vegetation indices  
    vegetation_features = ['NDVI', 'EVI', 'SAVI', 'NDRE']
    
    # Combine all candidate features
    candidate_features = priority_features + satellite_features + vegetation_features
    
    # Keep only existing features
    available_features = [f for f in candidate_features if f in train_df.columns]
    
    # Calculate correlations for validation
    correlations = train_df[available_features + ['yield']].corr()['yield'].abs()
    correlations = correlations.drop('yield').sort_values(ascending=False)
    
    # Select features above threshold
    strong_features = correlations[correlations >= min_correlation].index.tolist()
    
    logging.info(f"üìä SELECTED {len(strong_features)} STRONG FEATURES:")
    for i, feature in enumerate(strong_features[:15]):
        corr = correlations[feature]
        strength = "STRONG" if corr > 0.3 else "MEDIUM" if corr > 0.1 else "WEAK"
        logging.info(f"  {i+1:2d}. {feature:20s}: {corr:.4f} ({strength})")
    
    max_corr = correlations.max()
    if max_corr > 0.5:
        logging.info(f"‚úÖ Excellent correlation strength: {max_corr:.4f}")
    elif max_corr > 0.3:
        logging.info(f"‚úÖ Good correlation strength: {max_corr:.4f}")
    else:
        logging.warning(f"‚ö†Ô∏è Moderate correlation strength: {max_corr:.4f}")
    
    return strong_features

def prepare_normalized_data(train_df, test_df, features):
    """Prepare properly normalized data"""
    logging.info("üîß Preparing normalized data...")
    
    # Extract features and targets
    X_train = train_df[features].copy()
    X_test = test_df[features].copy()
    y_train = train_df['yield'].values
    y_test = test_df['yield'].values
    
    # Fill any remaining NaN with median
    for feature in features:
        if X_train[feature].isna().any():
            median_val = X_train[feature].median()
            X_train[feature] = X_train[feature].fillna(median_val)
            X_test[feature] = X_test[feature].fillna(median_val)
    
    # Convert to arrays
    X_train = X_train.values
    X_test = X_test.values
    
    # Final validation
    if np.isnan(X_train).any() or np.isnan(X_test).any() or np.isnan(y_train).any() or np.isnan(y_test).any():
        logging.error("‚ùå NaN values found!")
        return None, None, None, None, None, None
    
    # PROPER SCALING: Use MinMaxScaler for already normalized satellite bands
    feature_scaler = MinMaxScaler()
    X_train_scaled = feature_scaler.fit_transform(X_train)
    X_test_scaled = feature_scaler.transform(X_test)
    
    # Handle distribution shift with robust target scaling
    target_scaler = StandardScaler()
    y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()
    
    # Validation
    logging.info(f"‚úÖ Feature scaling: [{X_train_scaled.min():.4f}, {X_train_scaled.max():.4f}]")
    logging.info(f"‚úÖ Target scaling: Train mean={y_train_scaled.mean():.4f}, Test mean={y_test_scaled.mean():.4f}")
    
    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler

def create_optimized_tabular_model(input_dim):
    """Create optimized TABULAR model (not LSTM!)"""
    logging.info(f"üèóÔ∏è Creating TABULAR model for {input_dim} features...")
    
    model = Sequential([
        # Dense layers optimized for tabular data
        Dense(64, activation='relu', input_shape=(input_dim,)),
        BatchNormalization(),
        Dropout(0.3),
        
        Dense(32, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        
        Dense(16, activation='relu'),
        Dropout(0.1),
        
        Dense(8, activation='relu'),
        
        Dense(1)  # Output layer
    ])
    
    # Optimized compilation
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='mse',
        metrics=['mae']
    )
    
    logging.info(f"‚úÖ Tabular model created: {model.count_params()} parameters")
    logging.info("üéØ Architecture: Dense layers for tabular agricultural data")
    return model

def train_with_callbacks(model, X_train, y_train, X_val, y_val):
    """Train with proper callbacks"""
    logging.info("üöÄ Training optimized tabular model...")
    
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=25,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            'models/best_tabular_model.keras',
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=64,
        callbacks=callbacks,
        verbose=1
    )
    
    return model, history

def evaluate_comprehensive(model, X_test, y_test, target_scaler):
    """Comprehensive evaluation"""
    logging.info("üìä Evaluating tabular model...")
    
    # Predict
    y_pred_scaled = model.predict(X_test, verbose=0)
    
    # Inverse transform to original scale
    y_test_orig = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
    y_pred_orig = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # Calculate metrics
    mse = mean_squared_error(y_test_orig, y_pred_orig)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test_orig, y_pred_orig)
    r2 = r2_score(y_test_orig, y_pred_orig)
    
    # Additional metrics
    mape = np.mean(np.abs((y_test_orig - y_pred_orig) / y_test_orig)) * 100
    bias = np.mean(y_pred_orig - y_test_orig)
    
    logging.info(f"üéØ FINAL RESULTS:")
    logging.info(f"  R¬≤ Score: {r2:.4f}")
    logging.info(f"  RMSE: {rmse:.4f} bushels/acre")
    logging.info(f"  MAE: {mae:.4f} bushels/acre")
    logging.info(f"  MAPE: {mape:.2f}%")
    logging.info(f"  Bias: {bias:.4f} bushels/acre")
    
    # Success check
    if r2 > 0:
        logging.info(f"üéâ SUCCESS: POSITIVE R¬≤ ACHIEVED!")
    elif r2 > -0.2:
        logging.info(f"üìà MAJOR IMPROVEMENT: Close to positive R¬≤")
    else:
        logging.warning(f"‚ö†Ô∏è Still negative R¬≤, but better than before")
    
    # Create visualization
    plt.figure(figsize=(15, 10))
    
    # 1. Actual vs Predicted
    plt.subplot(2, 2, 1)
    plt.scatter(y_test_orig, y_pred_orig, alpha=0.6, s=30)
    plt.plot([y_test_orig.min(), y_test_orig.max()], 
             [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)
    plt.xlabel('Actual Yield (bushels/acre)')
    plt.ylabel('Predicted Yield (bushels/acre)')
    plt.title(f'Tabular Model Results (R¬≤ = {r2:.3f})')
    plt.grid(True, alpha=0.3)
    
    # 2. Residuals
    plt.subplot(2, 2, 2)
    residuals = y_test_orig - y_pred_orig
    plt.scatter(y_pred_orig, residuals, alpha=0.6, s=30)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted Yield')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.grid(True, alpha=0.3)
    
    # 3. Error distribution
    plt.subplot(2, 2, 3)
    plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')
    plt.axvline(x=0, color='r', linestyle='--')
    plt.xlabel('Prediction Error')
    plt.ylabel('Frequency')
    plt.title('Error Distribution')
    plt.grid(True, alpha=0.3)
    
    # 4. Performance summary
    plt.subplot(2, 2, 4)
    plt.axis('off')
    summary_text = f"""
    PROPER NORMALIZED TABULAR MODEL RESULTS
    
    ‚úÖ Satellite bands normalized (0-1)
    ‚úÖ Tabular architecture (not LSTM)
    ‚úÖ Strong features selected
    
    Performance Metrics:
    R¬≤ Score: {r2:.4f}
    RMSE: {rmse:.2f} bushels/acre
    MAE: {mae:.2f} bushels/acre
    MAPE: {mape:.2f}%
    Bias: {bias:.2f} bushels/acre
    
    Sample Size: {len(y_test_orig)}
    Mean Actual: {y_test_orig.mean():.2f}
    Mean Predicted: {y_pred_orig.mean():.2f}
    """
    
    plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes,
             fontsize=11, verticalalignment='top', 
             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig('reports/figures/proper_normalized_tabular_results.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return {'r2': r2, 'rmse': rmse, 'mae': mae, 'mape': mape, 'bias': bias}

def main():
    """Main function with all fixes applied"""
    try:
        logging.info("üöÄ STARTING PROPER NORMALIZED TABULAR MODEL...")
        
        # Load and fix data (critical normalization fixes)
        train_df, test_df = load_and_fix_data()
        
        # Select strong features
        features = select_strong_features(train_df, min_correlation=0.05)
        
        # Prepare normalized data
        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler = prepare_normalized_data(
            train_df, test_df, features
        )
        
        if X_train_scaled is None:
            logging.error("‚ùå Data preparation failed!")
            return None
        
        # Split for validation
        X_train_final, X_val, y_train_final, y_val = train_test_split(
            X_train_scaled, y_train_scaled, test_size=0.2, random_state=42
        )
        
        logging.info(f"üìä Data shapes: Train {X_train_final.shape}, Val {X_val.shape}, Test {X_test_scaled.shape}")
        
        # Create tabular model (NOT LSTM!)
        model = create_optimized_tabular_model(X_train_final.shape[1])
        
        # Train model
        model, history = train_with_callbacks(model, X_train_final, y_train_final, X_val, y_val)
        
        # Comprehensive evaluation
        results = evaluate_comprehensive(model, X_test_scaled, y_test_scaled, target_scaler)
        
        logging.info("üéâ PROPER NORMALIZED TABULAR MODEL COMPLETE!")
        logging.info(f"üèÜ Final R¬≤ Score: {results['r2']:.4f}")
        
        return results
        
    except Exception as e:
        logging.error(f"‚ùå Error: {str(e)}")
        raise

if __name__ == "__main__":
    results = main() 